{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "* NLP Tasks\n",
    "* Popular Techniques\n",
    "* Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Tasks:\n",
    "* Information extraction:\n",
    "    * Named entity recognition (NER)\n",
    "    * Coreference resolution\n",
    "    * Relationship extraction\n",
    "* Information retrieval\n",
    "* Sentiment analysis\n",
    "* Natural language generation\n",
    "* Machine translation\n",
    "* Recommender System\n",
    "* Question answering\n",
    "* Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Architecture for an Information Extraction System\n",
    "https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ie-architecture](https://www.nltk.org/images/ie-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"Joining / merging on duplicate keys can cause a returned frame that is the multiplication of the row \n",
    "dimensions, which may result in memory overflow. It is the user’ s responsibility to manage duplicate values \n",
    "in keys before joining large DataFrames.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Joining / merging on duplicate keys can cause a returned frame that is the multiplication of the row \\ndimensions, which may result in memory overflow.',\n",
       " 'It is the user’ s responsibility to manage duplicate values \\nin keys before joining large DataFrames.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbered list problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"1. Joining / merging on duplicate keys can cause a returned frame that is the multiplication of the row \n",
    "dimensions, which may result in memory overflow. 2. It is the user’ s responsibility to manage duplicate values \n",
    "in keys before joining large DataFrames.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.',\n",
       " 'Joining / merging on duplicate keys can cause a returned frame that is the multiplication of the row \\ndimensions, which may result in memory overflow.',\n",
       " '2.',\n",
       " 'It is the user’ s responsibility to manage duplicate values \\nin keys before joining large DataFrames.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions and solving of numbered list problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^(?:(?:(?:0?[13578]|1[02])(\\\\/|-|\\\\.)31)\\\\1|(?:(?:0?[13-9]|1[0-2])(\\\\/|-|\\\\.)(?:29|30)\\\\2))(?:(?:1[6-9]|[2-9]\\\\d)?\\\\d{2})$|^(?:0?2(\\\\/|-|\\\\.)29\\\\3(?:(?:(?:1[6-9]|[2-9]\\\\d)?(?:0[48]|[2468][048]|[13579][26])|(?:(?:16|[2468][048]|[3579][26])00))))$|^(?:(?:0?[1-9])|(?:1[0-2]))(\\\\/|-|\\\\.)(?:0?[1-9]|1\\\\d|2[0-8])\\\\4(?:(?:1[6-9]|[2-9]\\\\d)?\\\\d{2})$'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r\"^(?:(?:(?:0?[13578]|1[02])(\\/|-|\\.)31)\\1|(?:(?:0?[13-9]|1[0-2])(\\/|-|\\.)(?:29|30)\\2))(?:(?:1[6-9]|[2-9]\\d)?\\d{2})$|^(?:0?2(\\/|-|\\.)29\\3(?:(?:(?:1[6-9]|[2-9]\\d)?(?:0[48]|[2468][048]|[13579][26])|(?:(?:16|[2468][048]|[3579][26])00))))$|^(?:(?:0?[1-9])|(?:1[0-2]))(\\/|-|\\.)(?:0?[1-9]|1\\d|2[0-8])\\4(?:(?:1[6-9]|[2-9]\\d)?\\d{2})$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img\\wtf.jpg \"WTF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(re.match(r\"\\d+\\.\", \"4.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why we have to escape dot character?\n",
    "bool(re.match(r\"\\d+.\", \"4b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://regex101.com/r/F8dY80/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "2.\n"
     ]
    }
   ],
   "source": [
    "sent_text = nltk.sent_tokenize(s)\n",
    "# sent_text.append(\"4. New sentence\")\n",
    "\n",
    "for sent in sent_text:\n",
    "    if re.match(r\"\\d+\\.\", sent):\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _separate_by_sent(text: str):\n",
    "    sent_text = nltk.sent_tokenize(text)\n",
    "\n",
    "    for i, sent in enumerate(sent_text):\n",
    "        sent_text[i] = re.sub(\n",
    "            r\"\\d+\\.$\",\n",
    "            lambda x: '' if len(x.group()) < 5 else x.group(),\n",
    "            sent)\n",
    "\n",
    "    return [x for x in sent_text if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Joining / merging on duplicate keys can cause a returned frame that is the multiplication of the row \\ndimensions, which may result in memory overflow.',\n",
       " 'It is the user’ s responsibility to manage duplicate values \\nin keys before joining large DataFrames.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = _separate_by_sent(s)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Joining',\n",
       "  '/',\n",
       "  'merging',\n",
       "  'on',\n",
       "  'duplicate',\n",
       "  'keys',\n",
       "  'can',\n",
       "  'cause',\n",
       "  'a',\n",
       "  'returned',\n",
       "  'frame',\n",
       "  'that',\n",
       "  'is',\n",
       "  'the',\n",
       "  'multiplication',\n",
       "  'of',\n",
       "  'the',\n",
       "  'row',\n",
       "  'dimensions',\n",
       "  ',',\n",
       "  'which',\n",
       "  'may',\n",
       "  'result',\n",
       "  'in',\n",
       "  'memory',\n",
       "  'overflow',\n",
       "  '.'],\n",
       " ['It',\n",
       "  'is',\n",
       "  'the',\n",
       "  'user',\n",
       "  '’',\n",
       "  's',\n",
       "  'responsibility',\n",
       "  'to',\n",
       "  'manage',\n",
       "  'duplicate',\n",
       "  'values',\n",
       "  'in',\n",
       "  'keys',\n",
       "  'before',\n",
       "  'joining',\n",
       "  'large',\n",
       "  'DataFrames',\n",
       "  '.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_sent = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "tk_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Joining', 'VBG'),\n",
       " ('/', 'RP'),\n",
       " ('merging', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('duplicate', 'NN'),\n",
       " ('keys', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('cause', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('returned', 'JJ'),\n",
       " ('frame', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('multiplication', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('row', 'NN'),\n",
       " ('dimensions', 'NNS'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('may', 'MD'),\n",
       " ('result', 'VB'),\n",
       " ('in', 'IN'),\n",
       " ('memory', 'NN'),\n",
       " ('overflow', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_pos_sent = [nltk.pos_tag(sent) for sent in tk_sent]\n",
    "tk_pos_sent[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get POS by sentences not by tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_pos_sent[0] == [nltk.pos_tag([tk]) for tk in tk_sent[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys ---> key\n",
      "dimensions ---> dimension\n",
      "values ---> value\n",
      "keys ---> key\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "for sent in tk_sent:\n",
    "    for tk in sent:\n",
    "        tk = tk.lower()\n",
    "        lemma = lmtzr.lemmatize(tk)  # without POS\n",
    "#         print(f\"{tk} ---> {lemma}\")\n",
    "        if tk != lemma:\n",
    "            print(f\"{tk} ---> {lemma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wnpos(pos: str) -> str:\n",
    "    \"\"\"Transform nltk POS to wordnet POS.\"\"\"\n",
    "    pos = pos.lower()\n",
    "    wnpos = \"n\"\n",
    "\n",
    "    if pos.startswith(\"j\"):\n",
    "        wnpos = \"a\"\n",
    "    elif pos[0] in ('n', 'r', 'v'):\n",
    "        wnpos = pos[0]\n",
    "\n",
    "    return wnpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG v\n",
      "RP r\n",
      "VBG v\n",
      "IN n\n",
      "NN n\n",
      "NNS n\n",
      "MD n\n",
      "VB v\n",
      "DT n\n",
      "JJ a\n",
      "NN n\n",
      "WDT n\n",
      "VBZ v\n",
      "DT n\n",
      "NN n\n",
      "IN n\n",
      "DT n\n",
      "NN n\n",
      "NNS n\n",
      ", n\n",
      "WDT n\n",
      "MD n\n",
      "VB v\n",
      "IN n\n",
      "NN n\n",
      "NN n\n",
      ". n\n",
      "PRP n\n",
      "VBZ v\n",
      "DT n\n",
      "JJ a\n",
      "NNP n\n",
      "NN n\n",
      "NN n\n",
      "TO n\n",
      "VB v\n",
      "NN n\n",
      "NNS n\n",
      "IN n\n",
      "NNS n\n",
      "IN n\n",
      "VBG v\n",
      "JJ a\n",
      "NNP n\n",
      ". n\n"
     ]
    }
   ],
   "source": [
    "for sent in tk_pos_sent:\n",
    "    for tk, pos in sent:\n",
    "        print(pos, _wnpos(pos))\n",
    "#         tk = tk.lower()\n",
    "#         lemma = lmtzr.lemmatize(tk, _wnpos(pos))\n",
    "        \n",
    "#         if tk != lemma:\n",
    "#             print(f\"{tk} ---> {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joining ---> join\n",
      "/ ---> /\n",
      "merging ---> merg\n",
      "on ---> on\n",
      "duplicate ---> duplic\n",
      "keys ---> key\n",
      "can ---> can\n",
      "cause ---> caus\n",
      "a ---> a\n",
      "returned ---> return\n",
      "frame ---> frame\n",
      "that ---> that\n",
      "is ---> is\n",
      "the ---> the\n",
      "multiplication ---> multipl\n",
      "of ---> of\n",
      "the ---> the\n",
      "row ---> row\n",
      "dimensions ---> dimens\n",
      ", ---> ,\n",
      "which ---> which\n",
      "may ---> may\n",
      "result ---> result\n",
      "in ---> in\n",
      "memory ---> memori\n",
      "overflow ---> overflow\n",
      ". ---> .\n",
      "it ---> it\n",
      "is ---> is\n",
      "the ---> the\n",
      "user ---> user\n",
      "’ ---> ’\n",
      "s ---> s\n",
      "responsibility ---> respons\n",
      "to ---> to\n",
      "manage ---> manag\n",
      "duplicate ---> duplic\n",
      "values ---> valu\n",
      "in ---> in\n",
      "keys ---> key\n",
      "before ---> befor\n",
      "joining ---> join\n",
      "large ---> larg\n",
      "dataframes ---> datafram\n",
      ". ---> .\n"
     ]
    }
   ],
   "source": [
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "for sent in tk_sent:\n",
    "    for tk in sent:\n",
    "        tk = tk.lower()\n",
    "        stem = sno.stem(tk)\n",
    "        print(f\"{tk} ---> {stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Joining', '/')\n",
      "('/', 'merging')\n",
      "('merging', 'on')\n",
      "('on', 'duplicate')\n",
      "('duplicate', 'keys')\n",
      "('keys', 'can')\n",
      "('can', 'cause')\n",
      "('cause', 'a')\n",
      "('a', 'returned')\n",
      "('returned', 'frame')\n",
      "('frame', 'that')\n",
      "('that', 'is')\n",
      "('is', 'the')\n",
      "('the', 'multiplication')\n",
      "('multiplication', 'of')\n",
      "('of', 'the')\n",
      "('the', 'row')\n",
      "('row', 'dimensions')\n",
      "('dimensions', ',')\n",
      "(',', 'which')\n",
      "('which', 'may')\n",
      "('may', 'result')\n",
      "('result', 'in')\n",
      "('in', 'memory')\n",
      "('memory', 'overflow')\n",
      "('overflow', '.')\n",
      "('It', 'is')\n",
      "('is', 'the')\n",
      "('the', 'user')\n",
      "('user', '’')\n",
      "('’', 's')\n",
      "('s', 'responsibility')\n",
      "('responsibility', 'to')\n",
      "('to', 'manage')\n",
      "('manage', 'duplicate')\n",
      "('duplicate', 'values')\n",
      "('values', 'in')\n",
      "('in', 'keys')\n",
      "('keys', 'before')\n",
      "('before', 'joining')\n",
      "('joining', 'large')\n",
      "('large', 'DataFrames')\n",
      "('DataFrames', '.')\n"
     ]
    }
   ],
   "source": [
    "for sent in tk_sent:\n",
    "    n_grams = nltk.ngrams(sent, 2)\n",
    "    \n",
    "    for grams in n_grams:\n",
    "        print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img\\spacy.png \"spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. u.k. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "# \n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img\\spacy2.png \"spacy2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">But \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is starting from behind. The company made a late push\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "into hardware, and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "’s Siri, available on \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    iPhones\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "’s \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Alexa\n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "software, which runs on its \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Echo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Dot\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " devices, have clear leads in\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "consumer adoption.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"\"\"But Google is starting from behind. The company made a late push\n",
    "into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa\n",
    "software, which runs on its Echo and Dot devices, have clear leads in\n",
    "consumer adoption.\"\"\"\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True, options={\"distance\": 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"530\" height=\"257.0\" style=\"max-width: none; height: 257.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">sentence</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,122.0 C70,62.0 165.0,62.0 165.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,124.0 L62,112.0 78,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M310,122.0 C310,62.0 405.0,62.0 405.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310,124.0 L302,112.0 318,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M190,122.0 C190,2.0 410.0,2.0 410.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M410.0,124.0 L418.0,112.0 402.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"This is a sentence\")\n",
    "# doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={\"distance\": 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cloud.google.com/natural-language/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "* bag of words\n",
    "* TF-IDF\n",
    "* hash embeddings\n",
    "* word2vec, FastText, GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()   \n",
    "vect = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 0),\n",
       " ('document', 1),\n",
       " ('first', 2),\n",
       " ('is', 3),\n",
       " ('one', 4),\n",
       " ('second', 5),\n",
       " ('the', 6),\n",
       " ('third', 7),\n",
       " ('this', 8)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words / TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(use_idf=True)  # switcher\n",
    "bow = transformer.fit_transform(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438777</td>\n",
       "      <td>0.541977</td>\n",
       "      <td>0.438777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.853226</td>\n",
       "      <td>0.222624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288477</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438777</td>\n",
       "      <td>0.541977</td>\n",
       "      <td>0.438777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document     first        is       one    second       the  \\\n",
       "0  0.000000  0.438777  0.541977  0.438777  0.000000  0.000000  0.358729   \n",
       "1  0.000000  0.272301  0.000000  0.272301  0.000000  0.853226  0.222624   \n",
       "2  0.552805  0.000000  0.000000  0.000000  0.552805  0.000000  0.288477   \n",
       "3  0.000000  0.438777  0.541977  0.438777  0.000000  0.000000  0.358729   \n",
       "\n",
       "      third      this  \n",
       "0  0.000000  0.438777  \n",
       "1  0.000000  0.272301  \n",
       "2  0.552805  0.000000  \n",
       "3  0.000000  0.438777  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "\n",
    "inverted = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "df = pd.DataFrame(csr_matrix.todense(bow))\n",
    "df.columns = [inverted[col] for col in df.columns]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and         0.552805\n",
       "document    0.438777\n",
       "first       0.541977\n",
       "is          0.438777\n",
       "one         0.552805\n",
       "second      0.853226\n",
       "the         0.358729\n",
       "third       0.552805\n",
       "this        0.438777\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "and         0.552805\n",
       "document    0.438777\n",
       "first       0.541977\n",
       "is          0.438777\n",
       "one         0.552805\n",
       "second      0.853226\n",
       "the         0.358729\n",
       "third       0.552805\n",
       "this        0.438777\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook nlp_presentation.ipynb to script\n",
      "[NbConvertApp] Writing 6294 bytes to nlp_presentation.py\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system(u'jupyter nbconvert --to script nlp_presentation.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
